{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MGAN_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGs+isOBU89FY5jNcon2XT"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkwh_DjlmCyr",
        "outputId": "d7b8abe4-a7ae-47b3-b7f6-f17b94f64950"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import argparse\n",
        "import math\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/MyDrive/Colab_Notebooks')\n",
        "import models\n",
        "\n",
        "from argparse import ArgumentParser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LONWnk7EkuE4"
      },
      "source": [
        "batch_norm = partial(tf.compat.v1.layers.batch_normalization,\n",
        "                     momentum=0.99,\n",
        "                     trainable=True,\n",
        "                     epsilon=1e-5,\n",
        "                     scale=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfCdRQlxlFQB"
      },
      "source": [
        "class Prior(object):\n",
        "    def __init__(self, type):\n",
        "        self.type = type\n",
        "\n",
        "    def sample(self, shape):\n",
        "        if self.type == \"uniform\":\n",
        "            return np.random.uniform(-1.0, 1.0, shape)\n",
        "        else:\n",
        "            return np.random.normal(0, 1, shape)\n",
        "\n",
        "def conv_out_size_same(size, stride):\n",
        "    return int(math.ceil(float(size) / float(stride)))\n",
        "\n",
        "def make_batches(size, batch_size):\n",
        "    '''Returns a list of batch indices (tuples of indices).\n",
        "    '''\n",
        "    return [(i, min(size, i + batch_size)) for i in range(0, size, batch_size)]\n",
        "\n",
        "def create_image_grid(x, img_size, tile_shape):\n",
        "    assert (x.shape[0] == tile_shape[0] * tile_shape[1])\n",
        "    assert (x[0].shape == img_size)\n",
        "\n",
        "    img = np.zeros((img_size[0] * tile_shape[0] + tile_shape[0] - 1,\n",
        "                    img_size[1] * tile_shape[1] + tile_shape[1] - 1,\n",
        "                    3))\n",
        "\n",
        "    for t in range(x.shape[0]):\n",
        "        i, j = t // tile_shape[1], t % tile_shape[1]\n",
        "        img[i * img_size[0] + i : (i + 1) * img_size[0] + i, j * img_size[1] + j : (j + 1) * img_size[1] + j] = x[t]\n",
        "\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik-bA-NjlVWq"
      },
      "source": [
        "def linear(input, output_dim, scope='linear', stddev=0.01):\n",
        "    norm = tf.random_normal_initializer(stddev=stddev)\n",
        "    const = tf.constant_initializer(0.0)\n",
        "    with tf.variable_scope(scope):\n",
        "        w = tf.get_variable('weights', [input.get_shape()[1], output_dim], initializer=norm)\n",
        "        b = tf.get_variable('biases', [output_dim], initializer=const)\n",
        "        return tf.matmul(input, w) + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3CDU8DR6a1z"
      },
      "source": [
        "def conv2d(input_, output_dim,\n",
        "           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
        "           name=\"conv2d\"):\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable('weights', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
        "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "        # conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
        "\n",
        "        return tf.nn.bias_add(conv, biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeoQ1YNm5Ys5"
      },
      "source": [
        "def deconv2d(input_, output_shape,\n",
        "             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
        "             name=\"deconv2d\", with_w=False):\n",
        "    with tf.variable_scope(name):\n",
        "        # filter : [height, width, output_channels, in_channels]\n",
        "        w = tf.get_variable('weights', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
        "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
        "\n",
        "        try:\n",
        "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n",
        "                                            strides=[1, d_h, d_w, 1])\n",
        "\n",
        "        # Support for versions of TensorFlow before 0.7.0\n",
        "        except AttributeError:\n",
        "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n",
        "                                    strides=[1, d_h, d_w, 1])\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_shape[-1]],\n",
        "                                 initializer=tf.constant_initializer(0.0))\n",
        "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
        "\n",
        "        if with_w:\n",
        "            return deconv, w, biases\n",
        "        else:\n",
        "            return deconv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5OkeAuY6QPr"
      },
      "source": [
        "def lrelu(x, alpha=0.2):\n",
        "    return tf.maximum(x, alpha * x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "flygcpRlv7QU",
        "outputId": "03125894-5001-40e2-aa4a-9f324fd6389d"
      },
      "source": [
        "!pip install Pillow==5.0.0\n",
        "!pip install scipy==1.0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==5.0.0\n",
            "  Downloading Pillow-5.0.0.tar.gz (14.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2 MB 162 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow\n",
            "  Building wheel for Pillow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow: filename=Pillow-5.0.0-cp37-cp37m-linux_x86_64.whl size=1066918 sha256=50f85d86acba6c8e0ce539f86b6a22bf34cdd67f38c0b97977ee0d31784d3110\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/e9/3f/e1cf5b199032b95130c1e68222992ce83a78a51bdba0577a3b\n",
            "Successfully built Pillow\n",
            "Installing collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires pillow>=5.3.0, but you have pillow 5.0.0 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 5.0.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-5.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.0.1\n",
            "  Downloading scipy-1.0.1.tar.gz (15.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.5 MB 158 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: scipy\n",
            "  Building wheel for scipy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scipy: filename=scipy-1.0.1-cp37-cp37m-linux_x86_64.whl size=39978877 sha256=11944af8a70b6bf4c6f10e86b622fe5a469afd89b0e7dff3279e6f219eeac2b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/78/6c/81857cb7b297db9725704452fd48e7ef25e0fc395642aee63c\n",
            "Successfully built scipy\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc3 3.11.2 requires scipy>=1.2.0, but you have scipy 1.0.1 which is incompatible.\n",
            "plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.0.1 which is incompatible.\n",
            "cvxpy 1.0.31 requires scipy>=1.1.0, but you have scipy 1.0.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt3TDrww10xm",
        "outputId": "3e269a6c-43bc-4de8-e253-3ed647b28ef9"
      },
      "source": [
        "#from models import MGAN\n",
        "\n",
        "\n",
        "class MGAN(object):\n",
        "    \"\"\"Mixture Generative Adversarial Nets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_name='MGAN',\n",
        "                 beta=1.0,\n",
        "                 num_z=128,\n",
        "                 num_gens=4,\n",
        "                 d_batch_size=64,\n",
        "                 g_batch_size=32,\n",
        "                 z_prior=\"uniform\",\n",
        "                 same_input=True,\n",
        "                 learning_rate=0.0002,\n",
        "                 img_size=(32, 32, 3),  # (height, width, channels)\n",
        "                 num_conv_layers=3,\n",
        "                 num_gen_feature_maps=128,  # number of feature maps of generator\n",
        "                 num_dis_feature_maps=128,  # number of feature maps of discriminator\n",
        "                 sample_fp=None,\n",
        "                 sample_by_gen_fp=None,\n",
        "                 num_epochs=5,\n",
        "                 random_seed=6789):\n",
        "        self.beta = beta\n",
        "        self.num_z = num_z\n",
        "        self.num_gens = num_gens\n",
        "        self.d_batch_size = d_batch_size\n",
        "        self.g_batch_size = g_batch_size\n",
        "        self.z_prior = Prior(z_prior)\n",
        "        self.same_input = same_input\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.img_size = img_size\n",
        "        self.num_conv_layers = num_conv_layers\n",
        "        self.num_gen_feature_maps = num_gen_feature_maps\n",
        "        self.num_dis_feature_maps = num_dis_feature_maps\n",
        "        self.sample_fp = sample_fp\n",
        "        self.sample_by_gen_fp = sample_by_gen_fp\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "    def _init(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "        # TensorFlow's initialization\n",
        "        self.tf_graph = tf.Graph()\n",
        "        self.tf_config = tf.ConfigProto()\n",
        "        self.tf_config.gpu_options.allow_growth = True\n",
        "        self.tf_config.log_device_placement = False\n",
        "        self.tf_config.allow_soft_placement = True\n",
        "        self.tf_session = tf.Session(config=self.tf_config, graph=self.tf_graph)\n",
        "\n",
        "        np.random.seed(self.random_seed)\n",
        "        with self.tf_graph.as_default():\n",
        "            tf.set_random_seed(self.random_seed)\n",
        "\n",
        "    def _build_model(self):\n",
        "        arr = np.array([i // self.g_batch_size for i in range(self.g_batch_size * self.num_gens)])\n",
        "        d_mul_labels = tf.constant(arr, dtype=tf.int32)\n",
        "\n",
        "        self.x = tf.placeholder(tf.float32, [None,\n",
        "                                             self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                                name=\"real_data\")\n",
        "        self.z = tf.placeholder(tf.float32, [self.g_batch_size * self.num_gens, self.num_z], name='noise')\n",
        "\n",
        "        # create generator G\n",
        "        self.g = self._create_generator(self.z)\n",
        "\n",
        "        # create sampler to generate samples\n",
        "        self.sampler = self._create_generator(self.z, train=False, reuse=True)\n",
        "\n",
        "        # create discriminator D\n",
        "        d_bin_x_logits, d_mul_x_logits = self._create_discriminator(self.x)\n",
        "        d_bin_g_logits, d_mul_g_logits = self._create_discriminator(self.g, reuse=True)\n",
        "\n",
        "        # define loss functions\n",
        "        self.d_bin_x_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=d_bin_x_logits, labels=tf.ones_like(d_bin_x_logits)),\n",
        "            name='d_bin_x_loss')\n",
        "        self.d_bin_g_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=d_bin_g_logits, labels=tf.zeros_like(d_bin_g_logits)),\n",
        "            name='d_bin_g_loss')\n",
        "        self.d_bin_loss = tf.add(self.d_bin_x_loss, self.d_bin_g_loss, name='d_bin_loss')\n",
        "        self.d_mul_loss = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                logits=d_mul_g_logits, labels=d_mul_labels),\n",
        "            name=\"d_mul_loss\")\n",
        "        self.d_loss = tf.add(self.d_bin_loss, self.d_mul_loss, name=\"d_loss\")\n",
        "\n",
        "        self.g_bin_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=d_bin_g_logits, labels=tf.ones_like(d_bin_g_logits)),\n",
        "            name=\"g_bin_loss\")\n",
        "        self.g_mul_loss = tf.multiply(self.beta, self.d_mul_loss, name='g_mul_loss')\n",
        "        self.g_loss = tf.add(self.g_bin_loss, self.g_mul_loss, name=\"g_loss\")\n",
        "\n",
        "        # create optimizers\n",
        "        self.d_opt = self._create_optimizer(self.d_loss, scope='discriminator',\n",
        "                                            lr=self.learning_rate)\n",
        "        self.g_opt = self._create_optimizer(self.g_loss, scope='generator',\n",
        "                                            lr=self.learning_rate)\n",
        "\n",
        "    def _create_generator(self, z, train=True, reuse=False, name=\"generator\"):\n",
        "        out_size = [(conv_out_size_same(self.img_size[0], 2),\n",
        "                     conv_out_size_same(self.img_size[1], 2),\n",
        "                     self.num_gen_feature_maps)]\n",
        "        for i in range(self.num_conv_layers - 1):\n",
        "            out_size = [(conv_out_size_same(out_size[0][0], 2),\n",
        "                         conv_out_size_same(out_size[0][1], 2),\n",
        "                         out_size[0][2] * 2)] + out_size\n",
        "\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            if reuse:\n",
        "                scope.reuse_variables()\n",
        "\n",
        "            z_split = tf.split(z, self.num_gens, axis=0)\n",
        "            h0 = []\n",
        "            for i, var in enumerate(z_split):\n",
        "                h0.append(tf.nn.relu(batch_norm(linear(var, out_size[0][0] * out_size[0][1] * out_size[0][2],\n",
        "                                                       scope='g_h0_linear{}'.format(i), stddev=0.02),\n",
        "                                                training=train #,scope=\"g_h0_bn{}\".format(i)\n",
        "                                                ),\n",
        "                                     name=\"g_h0_relu{}\".format(i)))\n",
        "\n",
        "            h = []\n",
        "            for var in h0:\n",
        "                h.append(tf.reshape(var, [self.g_batch_size, out_size[0][0], out_size[0][1], out_size[0][2]]))\n",
        "            h = tf.concat(h, axis=0, name=\"g_h0_relu\")\n",
        "\n",
        "            for i in range(1, self.num_conv_layers):\n",
        "                h = tf.nn.relu(\n",
        "                    batch_norm(\n",
        "                        deconv2d(h,\n",
        "                                 [self.g_batch_size  * self.num_gens, out_size[i][0], out_size[i][1], out_size[i][2]],\n",
        "                                 stddev=0.02, name=\"g_h{}_deconv\".format(i)),\n",
        "                        training=train,\n",
        "                        center=False\n",
        "                        #,scope=\"g_h{}_bn\".format(i)\n",
        "                        ),\n",
        "                    name=\"g_h{}_relu\".format(i))\n",
        "\n",
        "            g_out = tf.nn.tanh(\n",
        "                deconv2d(h,\n",
        "                         [self.g_batch_size * self.num_gens, self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                         stddev=0.02, name=\"g_out_deconv\"),\n",
        "                name=\"g_out_tanh\")\n",
        "            return g_out\n",
        "\n",
        "    def _create_discriminator(self, x, train=True, reuse=False, name=\"discriminator\"):\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            if reuse:\n",
        "                scope.reuse_variables()\n",
        "\n",
        "            h = x\n",
        "            for i in range(self.num_conv_layers):\n",
        "                h = lrelu(batch_norm(conv2d(h, self.num_dis_feature_maps * (2 ** i),\n",
        "                                            stddev=0.02, name=\"d_h{}_conv\".format(i)),\n",
        "                                     training=train\n",
        "                                     #,\n",
        "                                     #scope=\"d_bn{}\".format(i)\n",
        "                                     ))\n",
        "\n",
        "            dim = h.get_shape()[1:].num_elements()\n",
        "            h = tf.reshape(h, [-1, dim])\n",
        "            d_bin_logits = linear(h, 1, scope='d_bin_logits')\n",
        "            d_mul_logits = linear(h, self.num_gens, scope='d_mul_logits')\n",
        "        return d_bin_logits, d_mul_logits\n",
        "\n",
        "    def _create_optimizer(self, loss, scope, lr):\n",
        "        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
        "        opt = tf.train.AdamOptimizer(lr, beta1=0.5)\n",
        "        grads = opt.compute_gradients(loss, var_list=params)\n",
        "        train_op = opt.apply_gradients(grads)\n",
        "        return train_op\n",
        "\n",
        "    def fit(self, x):\n",
        "        if (not hasattr(self, 'epoch')) or self.epoch == 0:\n",
        "            self._init()\n",
        "            with self.tf_graph.as_default():\n",
        "                self._build_model()\n",
        "                self.tf_session.run(tf.global_variables_initializer())\n",
        "\n",
        "        num_data = x.shape[0] - x.shape[0] % self.d_batch_size\n",
        "        batches = make_batches(num_data, self.d_batch_size)\n",
        "        best_is = 0.0\n",
        "        while (self.epoch < self.num_epochs):\n",
        "            for batch_idx, (batch_start, batch_end) in enumerate(batches):\n",
        "                batch_size = batch_end - batch_start\n",
        "\n",
        "                x_batch = x[batch_start:batch_end]\n",
        "                if self.same_input:\n",
        "                    z_batch = self.z_prior.sample([self.g_batch_size, self.num_z]).astype(np.float32)\n",
        "                    z_batch = np.vstack([z_batch] * self.num_gens)\n",
        "                else:\n",
        "                    z_batch = self.z_prior.sample([self.g_batch_size * self.num_gens, self.num_z]).astype(np.float32)\n",
        "\n",
        "                # update discriminator D\n",
        "                d_bin_loss, d_mul_loss, d_loss, _ = self.tf_session.run(\n",
        "                    [self.d_bin_loss, self.d_mul_loss, self.d_loss, self.d_opt],\n",
        "                    feed_dict={self.x: x_batch, self.z: z_batch})\n",
        "\n",
        "                # update generator G\n",
        "                g_bin_loss, g_mul_loss, g_loss, _ = self.tf_session.run(\n",
        "                    [self.g_bin_loss, self.g_mul_loss, self.g_loss, self.g_opt],\n",
        "                    feed_dict={self.z: z_batch})\n",
        "\n",
        "            self.epoch += 1\n",
        "            print(\"Epoch: [%4d/%4d] d_bin_loss: %.5f, d_mul_loss: %.5f, d_loss: %.5f,\"\n",
        "                  \" g_bin_loss: %.5f, g_mul_loss: %.5f, g_loss: %.5f\" % (self.epoch, self.num_epochs,\n",
        "                                d_bin_loss, d_mul_loss, d_loss, g_bin_loss, g_mul_loss, g_loss))\n",
        "            self._samples(self.sample_fp.format(epoch=self.epoch+1))\n",
        "            self._samples_by_gen(self.sample_by_gen_fp.format(epoch=self.epoch+1))\n",
        "\n",
        "    def _generate(self, num_samples=100):\n",
        "        sess = self.tf_session\n",
        "        batch_size = self.g_batch_size * self.num_gens\n",
        "        num = ((num_samples - 1) // batch_size + 1) * batch_size\n",
        "        z = self.z_prior.sample([num, self.num_z]).astype(np.float32)\n",
        "        x = np.zeros([num, self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                     dtype=np.float32)\n",
        "        batches = make_batches(num, batch_size)\n",
        "        for batch_idx, (batch_start, batch_end) in enumerate(batches):\n",
        "            z_batch = z[batch_start:batch_end]\n",
        "            x[batch_start:batch_end] = sess.run(self.sampler,\n",
        "                                                feed_dict={self.z: z_batch})\n",
        "        idx = np.random.permutation(num)[:num_samples]\n",
        "        x = (x[idx] + 1.0) / 2.0\n",
        "        return x\n",
        "\n",
        "    def _samples(self, filepath, tile_shape=(10, 10)):\n",
        "        if not os.path.exists(os.path.dirname(filepath)):\n",
        "            os.makedirs(os.path.dirname(filepath))\n",
        "\n",
        "        num_samples = tile_shape[0] * tile_shape[1]\n",
        "        x = self._generate(num_samples)\n",
        "        imgs = create_image_grid(x, img_size=self.img_size, tile_shape=tile_shape)\n",
        "        import scipy.misc\n",
        "        scipy.misc.imsave(filepath, imgs)\n",
        "\n",
        "    def _samples_by_gen(self, filepath):\n",
        "        if not os.path.exists(os.path.dirname(filepath)):\n",
        "            os.makedirs(os.path.dirname(filepath))\n",
        "\n",
        "        num_samples = self.num_gens * 10\n",
        "        tile_shape = (self.num_gens, 10)\n",
        "\n",
        "        sess = self.tf_session\n",
        "        img_per_gen = num_samples // self.num_gens\n",
        "        x = np.zeros([num_samples, self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                     dtype=np.float32)\n",
        "        for i in range(0, img_per_gen, self.g_batch_size):\n",
        "            z_batch = self.z_prior.sample([self.g_batch_size * self.num_gens, self.num_z]).astype(np.float32)\n",
        "            samples = sess.run(self.sampler, feed_dict={self.z: z_batch})\n",
        "\n",
        "            for gen in range(self.num_gens):\n",
        "                x[gen * img_per_gen + i:gen * img_per_gen + min(i + self.g_batch_size, img_per_gen)] = \\\n",
        "                    samples[\n",
        "                    gen * self.g_batch_size:gen * self.g_batch_size + min(self.g_batch_size, img_per_gen)]\n",
        "\n",
        "        x = (x + 1.0) / 2.0\n",
        "        imgs = create_image_grid(x, img_size=self.img_size, tile_shape=tile_shape)\n",
        "        import scipy.misc\n",
        "        scipy.misc.imsave(filepath, imgs)\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    tmp = pickle.load(open(\"/content/drive/MyDrive/Colab_Notebooks/cifar10_train.pkl\", \"rb\"))\n",
        "    x_train = tmp['data'].astype(np.float32).reshape([-1, 32, 32, 3]) / 127.5 - 1.\n",
        "    model = MGAN(\n",
        "        num_z=FLAGS.num_z,\n",
        "        beta=FLAGS.beta,\n",
        "        num_gens=FLAGS.num_gens,\n",
        "        d_batch_size=FLAGS.d_batch_size,\n",
        "        g_batch_size=FLAGS.g_batch_size,\n",
        "        z_prior=FLAGS.z_prior,\n",
        "        learning_rate=FLAGS.learning_rate,\n",
        "        img_size=(32, 32, 3),\n",
        "        num_conv_layers=FLAGS.num_conv_layers,\n",
        "        num_gen_feature_maps=FLAGS.num_gen_feature_maps,\n",
        "        num_dis_feature_maps=FLAGS.num_dis_feature_maps,\n",
        "        num_epochs=FLAGS.num_epochs,\n",
        "        sample_fp=\"samples/samples_{epoch:04d}.png\",\n",
        "        sample_by_gen_fp=\"samples_by_gen/samples_{epoch:04d}.png\",\n",
        "        random_seed=6789)\n",
        "    model.fit(x_train)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--num_z', type=int, default=100,\n",
        "                        help='Number of latent units.')\n",
        "    parser.add_argument('--beta', type=float, default=0.01,\n",
        "                        help='Diversity parameter beta.')\n",
        "    parser.add_argument('--num_gens', type=int, default=10,\n",
        "                        help='Number of generators.')\n",
        "    parser.add_argument('--d_batch_size', type=int, default=64,\n",
        "                        help='Minibatch size for the discriminator.')\n",
        "    parser.add_argument('--g_batch_size', type=int, default=12,\n",
        "                        help='Minibatch size for the generators.')\n",
        "    parser.add_argument('--z_prior', type=str, default=\"uniform\",\n",
        "                        help='Prior distribution of the noise (uniform/gaussian).')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.0002,\n",
        "                        help='Learning rate.')\n",
        "    parser.add_argument('--num_conv_layers', type=int, default=3,\n",
        "                        help='Number of convolutional layers.')\n",
        "    parser.add_argument('--num_gen_feature_maps', type=int, default=128,\n",
        "                        help='Number of feature maps of Generator.')\n",
        "    parser.add_argument('--num_dis_feature_maps', type=int, default=128,\n",
        "                        help='Number of feature maps of Discriminator.')\n",
        "    parser.add_argument('--num_epochs', type=int, default=500,\n",
        "                        help='Number of epochs.')\n",
        "    FLAGS, unparsed = parser.parse_known_args()\n",
        "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/normalization.py:308: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  '`tf.layers.batch_normalization` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "W0809 04:12:26.687870 140590547040128 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [   1/ 500] d_bin_loss: 0.56341, d_mul_loss: 1.27234, d_loss: 1.83576, g_bin_loss: 2.48522, g_mul_loss: 0.01148, g_loss: 2.49669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:241: DeprecationWarning:     `imsave` is deprecated!\n",
            "    `imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imwrite`` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:265: DeprecationWarning:     `imsave` is deprecated!\n",
            "    `imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imwrite`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [   2/ 500] d_bin_loss: 0.46849, d_mul_loss: 0.57453, d_loss: 1.04302, g_bin_loss: 2.56271, g_mul_loss: 0.00420, g_loss: 2.56691\n",
            "Epoch: [   3/ 500] d_bin_loss: 0.54170, d_mul_loss: 0.22021, d_loss: 0.76191, g_bin_loss: 2.25133, g_mul_loss: 0.00128, g_loss: 2.25261\n",
            "Epoch: [   4/ 500] d_bin_loss: 0.56257, d_mul_loss: 0.06653, d_loss: 0.62909, g_bin_loss: 2.15285, g_mul_loss: 0.00037, g_loss: 2.15322\n",
            "Epoch: [   5/ 500] d_bin_loss: 1.23974, d_mul_loss: 0.10832, d_loss: 1.34806, g_bin_loss: 6.11617, g_mul_loss: 0.00036, g_loss: 6.11653\n",
            "Epoch: [   6/ 500] d_bin_loss: 0.50384, d_mul_loss: 0.03908, d_loss: 0.54291, g_bin_loss: 1.43074, g_mul_loss: 0.00023, g_loss: 1.43097\n",
            "Epoch: [   7/ 500] d_bin_loss: 0.70218, d_mul_loss: 0.02532, d_loss: 0.72750, g_bin_loss: 1.27874, g_mul_loss: 0.00011, g_loss: 1.27885\n",
            "Epoch: [   8/ 500] d_bin_loss: 0.83135, d_mul_loss: 0.01071, d_loss: 0.84206, g_bin_loss: 1.22190, g_mul_loss: 0.00024, g_loss: 1.22214\n",
            "Epoch: [   9/ 500] d_bin_loss: 0.38235, d_mul_loss: 0.02239, d_loss: 0.40474, g_bin_loss: 2.35530, g_mul_loss: 0.00009, g_loss: 2.35538\n",
            "Epoch: [  10/ 500] d_bin_loss: 0.57666, d_mul_loss: 0.01589, d_loss: 0.59255, g_bin_loss: 2.88557, g_mul_loss: 0.00011, g_loss: 2.88568\n",
            "Epoch: [  11/ 500] d_bin_loss: 0.41962, d_mul_loss: 0.01569, d_loss: 0.43531, g_bin_loss: 2.82520, g_mul_loss: 0.00011, g_loss: 2.82531\n",
            "Epoch: [  12/ 500] d_bin_loss: 0.38218, d_mul_loss: 0.04293, d_loss: 0.42511, g_bin_loss: 2.37345, g_mul_loss: 0.00011, g_loss: 2.37356\n",
            "Epoch: [  13/ 500] d_bin_loss: 0.40956, d_mul_loss: 0.04472, d_loss: 0.45428, g_bin_loss: 2.66748, g_mul_loss: 0.00006, g_loss: 2.66755\n",
            "Epoch: [  14/ 500] d_bin_loss: 0.44571, d_mul_loss: 0.00466, d_loss: 0.45037, g_bin_loss: 2.87060, g_mul_loss: 0.00004, g_loss: 2.87064\n",
            "Epoch: [  15/ 500] d_bin_loss: 0.56293, d_mul_loss: 0.02322, d_loss: 0.58615, g_bin_loss: 2.08450, g_mul_loss: 0.00006, g_loss: 2.08456\n",
            "Epoch: [  16/ 500] d_bin_loss: 0.37215, d_mul_loss: 0.00847, d_loss: 0.38062, g_bin_loss: 3.06534, g_mul_loss: 0.00004, g_loss: 3.06537\n",
            "Epoch: [  17/ 500] d_bin_loss: 0.29193, d_mul_loss: 0.00785, d_loss: 0.29977, g_bin_loss: 2.71032, g_mul_loss: 0.00003, g_loss: 2.71035\n",
            "Epoch: [  18/ 500] d_bin_loss: 0.96523, d_mul_loss: 0.00930, d_loss: 0.97453, g_bin_loss: 3.32607, g_mul_loss: 0.00008, g_loss: 3.32616\n",
            "Epoch: [  19/ 500] d_bin_loss: 0.32954, d_mul_loss: 0.03247, d_loss: 0.36202, g_bin_loss: 2.39131, g_mul_loss: 0.00005, g_loss: 2.39136\n",
            "Epoch: [  20/ 500] d_bin_loss: 0.62180, d_mul_loss: 0.01178, d_loss: 0.63358, g_bin_loss: 4.31333, g_mul_loss: 0.00005, g_loss: 4.31338\n",
            "Epoch: [  21/ 500] d_bin_loss: 0.37351, d_mul_loss: 0.00521, d_loss: 0.37872, g_bin_loss: 3.07837, g_mul_loss: 0.00003, g_loss: 3.07840\n",
            "Epoch: [  22/ 500] d_bin_loss: 0.49587, d_mul_loss: 0.04375, d_loss: 0.53962, g_bin_loss: 3.44311, g_mul_loss: 0.00012, g_loss: 3.44323\n",
            "Epoch: [  23/ 500] d_bin_loss: 0.37555, d_mul_loss: 0.00701, d_loss: 0.38256, g_bin_loss: 2.48107, g_mul_loss: 0.00002, g_loss: 2.48109\n",
            "Epoch: [  24/ 500] d_bin_loss: 0.45364, d_mul_loss: 0.00647, d_loss: 0.46011, g_bin_loss: 3.26905, g_mul_loss: 0.00003, g_loss: 3.26907\n",
            "Epoch: [  25/ 500] d_bin_loss: 0.28562, d_mul_loss: 0.02141, d_loss: 0.30703, g_bin_loss: 2.91167, g_mul_loss: 0.00007, g_loss: 2.91174\n",
            "Epoch: [  26/ 500] d_bin_loss: 0.21892, d_mul_loss: 0.00372, d_loss: 0.22264, g_bin_loss: 3.51041, g_mul_loss: 0.00003, g_loss: 3.51045\n",
            "Epoch: [  27/ 500] d_bin_loss: 0.30814, d_mul_loss: 0.00604, d_loss: 0.31418, g_bin_loss: 3.36007, g_mul_loss: 0.00003, g_loss: 3.36009\n",
            "Epoch: [  28/ 500] d_bin_loss: 0.39000, d_mul_loss: 0.00463, d_loss: 0.39463, g_bin_loss: 3.35710, g_mul_loss: 0.00004, g_loss: 3.35714\n",
            "Epoch: [  29/ 500] d_bin_loss: 0.25501, d_mul_loss: 0.00183, d_loss: 0.25684, g_bin_loss: 2.82390, g_mul_loss: 0.00001, g_loss: 2.82391\n",
            "Epoch: [  30/ 500] d_bin_loss: 0.17176, d_mul_loss: 0.02407, d_loss: 0.19584, g_bin_loss: 3.57051, g_mul_loss: 0.00005, g_loss: 3.57056\n",
            "Epoch: [  31/ 500] d_bin_loss: 0.33141, d_mul_loss: 0.00560, d_loss: 0.33701, g_bin_loss: 4.19710, g_mul_loss: 0.00003, g_loss: 4.19713\n",
            "Epoch: [  32/ 500] d_bin_loss: 0.34052, d_mul_loss: 0.00277, d_loss: 0.34328, g_bin_loss: 3.28722, g_mul_loss: 0.00001, g_loss: 3.28723\n",
            "Epoch: [  33/ 500] d_bin_loss: 0.31092, d_mul_loss: 0.00301, d_loss: 0.31393, g_bin_loss: 2.84453, g_mul_loss: 0.00002, g_loss: 2.84455\n",
            "Epoch: [  34/ 500] d_bin_loss: 0.19840, d_mul_loss: 0.00300, d_loss: 0.20140, g_bin_loss: 3.16725, g_mul_loss: 0.00002, g_loss: 3.16727\n",
            "Epoch: [  35/ 500] d_bin_loss: 0.17859, d_mul_loss: 0.00125, d_loss: 0.17984, g_bin_loss: 3.46451, g_mul_loss: 0.00001, g_loss: 3.46452\n",
            "Epoch: [  36/ 500] d_bin_loss: 0.20824, d_mul_loss: 0.00199, d_loss: 0.21022, g_bin_loss: 3.89792, g_mul_loss: 0.00002, g_loss: 3.89794\n",
            "Epoch: [  37/ 500] d_bin_loss: 0.21407, d_mul_loss: 0.03000, d_loss: 0.24407, g_bin_loss: 3.54259, g_mul_loss: 0.00000, g_loss: 3.54260\n",
            "Epoch: [  38/ 500] d_bin_loss: 0.15036, d_mul_loss: 0.00148, d_loss: 0.15184, g_bin_loss: 3.88329, g_mul_loss: 0.00001, g_loss: 3.88330\n",
            "Epoch: [  39/ 500] d_bin_loss: 0.28976, d_mul_loss: 0.01858, d_loss: 0.30834, g_bin_loss: 3.84987, g_mul_loss: 0.00003, g_loss: 3.84990\n",
            "Epoch: [  40/ 500] d_bin_loss: 0.24174, d_mul_loss: 0.00313, d_loss: 0.24487, g_bin_loss: 3.42241, g_mul_loss: 0.00001, g_loss: 3.42242\n",
            "Epoch: [  41/ 500] d_bin_loss: 0.10022, d_mul_loss: 0.00046, d_loss: 0.10068, g_bin_loss: 4.50140, g_mul_loss: 0.00000, g_loss: 4.50140\n",
            "Epoch: [  42/ 500] d_bin_loss: 1.68401, d_mul_loss: 0.04773, d_loss: 1.73174, g_bin_loss: 0.13398, g_mul_loss: 0.00226, g_loss: 0.13624\n",
            "Epoch: [  43/ 500] d_bin_loss: 0.05861, d_mul_loss: 0.00506, d_loss: 0.06367, g_bin_loss: 4.46836, g_mul_loss: 0.00002, g_loss: 4.46838\n",
            "Epoch: [  44/ 500] d_bin_loss: 0.26110, d_mul_loss: 0.02009, d_loss: 0.28118, g_bin_loss: 3.82609, g_mul_loss: 0.00003, g_loss: 3.82611\n",
            "Epoch: [  45/ 500] d_bin_loss: 0.39998, d_mul_loss: 0.00458, d_loss: 0.40456, g_bin_loss: 4.56820, g_mul_loss: 0.00002, g_loss: 4.56822\n",
            "Epoch: [  46/ 500] d_bin_loss: 0.09485, d_mul_loss: 0.00046, d_loss: 0.09530, g_bin_loss: 4.57206, g_mul_loss: 0.00000, g_loss: 4.57206\n",
            "Epoch: [  47/ 500] d_bin_loss: 0.11553, d_mul_loss: 0.00089, d_loss: 0.11642, g_bin_loss: 4.70471, g_mul_loss: 0.00001, g_loss: 4.70471\n",
            "Epoch: [  48/ 500] d_bin_loss: 0.10361, d_mul_loss: 0.00999, d_loss: 0.11360, g_bin_loss: 4.44852, g_mul_loss: 0.00002, g_loss: 4.44854\n",
            "Epoch: [  49/ 500] d_bin_loss: 0.05001, d_mul_loss: 0.00307, d_loss: 0.05308, g_bin_loss: 4.99981, g_mul_loss: 0.00002, g_loss: 4.99982\n",
            "Epoch: [  50/ 500] d_bin_loss: 0.21549, d_mul_loss: 0.00330, d_loss: 0.21879, g_bin_loss: 3.42951, g_mul_loss: 0.00002, g_loss: 3.42953\n",
            "Epoch: [  51/ 500] d_bin_loss: 1.38460, d_mul_loss: 0.03854, d_loss: 1.42315, g_bin_loss: 6.59433, g_mul_loss: 0.00002, g_loss: 6.59436\n",
            "Epoch: [  52/ 500] d_bin_loss: 0.13788, d_mul_loss: 0.04458, d_loss: 0.18247, g_bin_loss: 4.44060, g_mul_loss: 0.00001, g_loss: 4.44061\n",
            "Epoch: [  53/ 500] d_bin_loss: 0.07734, d_mul_loss: 0.00445, d_loss: 0.08179, g_bin_loss: 4.91443, g_mul_loss: 0.00001, g_loss: 4.91444\n",
            "Epoch: [  54/ 500] d_bin_loss: 0.15622, d_mul_loss: 0.00339, d_loss: 0.15962, g_bin_loss: 4.24613, g_mul_loss: 0.00002, g_loss: 4.24615\n",
            "Epoch: [  55/ 500] d_bin_loss: 1.30868, d_mul_loss: 0.01183, d_loss: 1.32052, g_bin_loss: 0.92451, g_mul_loss: 0.00003, g_loss: 0.92454\n",
            "Epoch: [  56/ 500] d_bin_loss: 0.23347, d_mul_loss: 0.00138, d_loss: 0.23484, g_bin_loss: 4.76733, g_mul_loss: 0.00001, g_loss: 4.76734\n",
            "Epoch: [  57/ 500] d_bin_loss: 0.02926, d_mul_loss: 0.00026, d_loss: 0.02952, g_bin_loss: 5.10982, g_mul_loss: 0.00000, g_loss: 5.10982\n",
            "Epoch: [  58/ 500] d_bin_loss: 0.07695, d_mul_loss: 0.00010, d_loss: 0.07706, g_bin_loss: 5.05811, g_mul_loss: 0.00000, g_loss: 5.05811\n",
            "Epoch: [  59/ 500] d_bin_loss: 0.02329, d_mul_loss: 0.00048, d_loss: 0.02377, g_bin_loss: 5.09393, g_mul_loss: 0.00000, g_loss: 5.09393\n",
            "Epoch: [  60/ 500] d_bin_loss: 0.12009, d_mul_loss: 0.00250, d_loss: 0.12259, g_bin_loss: 4.37084, g_mul_loss: 0.00001, g_loss: 4.37085\n",
            "Epoch: [  61/ 500] d_bin_loss: 0.24454, d_mul_loss: 0.00145, d_loss: 0.24599, g_bin_loss: 3.70792, g_mul_loss: 0.00001, g_loss: 3.70793\n",
            "Epoch: [  62/ 500] d_bin_loss: 0.38255, d_mul_loss: 0.01308, d_loss: 0.39563, g_bin_loss: 6.95380, g_mul_loss: 0.00002, g_loss: 6.95383\n",
            "Epoch: [  63/ 500] d_bin_loss: 0.10348, d_mul_loss: 0.00196, d_loss: 0.10544, g_bin_loss: 5.11313, g_mul_loss: 0.00001, g_loss: 5.11314\n",
            "Epoch: [  64/ 500] d_bin_loss: 0.04664, d_mul_loss: 0.00021, d_loss: 0.04685, g_bin_loss: 5.45564, g_mul_loss: 0.00000, g_loss: 5.45564\n",
            "Epoch: [  65/ 500] d_bin_loss: 0.05843, d_mul_loss: 0.00442, d_loss: 0.06285, g_bin_loss: 4.88330, g_mul_loss: 0.00001, g_loss: 4.88331\n",
            "Epoch: [  66/ 500] d_bin_loss: 0.10856, d_mul_loss: 0.00030, d_loss: 0.10886, g_bin_loss: 5.23384, g_mul_loss: 0.00000, g_loss: 5.23384\n",
            "Epoch: [  67/ 500] d_bin_loss: 0.05358, d_mul_loss: 0.00008, d_loss: 0.05366, g_bin_loss: 5.24545, g_mul_loss: 0.00000, g_loss: 5.24545\n",
            "Epoch: [  68/ 500] d_bin_loss: 0.06443, d_mul_loss: 0.00255, d_loss: 0.06698, g_bin_loss: 4.75981, g_mul_loss: 0.00001, g_loss: 4.75982\n",
            "Epoch: [  69/ 500] d_bin_loss: 0.11289, d_mul_loss: 0.00045, d_loss: 0.11333, g_bin_loss: 4.81064, g_mul_loss: 0.00000, g_loss: 4.81064\n",
            "Epoch: [  70/ 500] d_bin_loss: 0.14944, d_mul_loss: 0.00090, d_loss: 0.15034, g_bin_loss: 4.99362, g_mul_loss: 0.00001, g_loss: 4.99363\n",
            "Epoch: [  71/ 500] d_bin_loss: 0.06202, d_mul_loss: 0.03974, d_loss: 0.10176, g_bin_loss: 4.46421, g_mul_loss: 0.00000, g_loss: 4.46421\n",
            "Epoch: [  72/ 500] d_bin_loss: 0.06288, d_mul_loss: 0.00261, d_loss: 0.06548, g_bin_loss: 5.06120, g_mul_loss: 0.00001, g_loss: 5.06122\n",
            "Epoch: [  73/ 500] d_bin_loss: 0.16203, d_mul_loss: 0.00130, d_loss: 0.16333, g_bin_loss: 4.37456, g_mul_loss: 0.00001, g_loss: 4.37457\n",
            "Epoch: [  74/ 500] d_bin_loss: 0.08600, d_mul_loss: 0.00034, d_loss: 0.08634, g_bin_loss: 4.85485, g_mul_loss: 0.00000, g_loss: 4.85485\n",
            "Epoch: [  75/ 500] d_bin_loss: 0.04146, d_mul_loss: 0.00228, d_loss: 0.04374, g_bin_loss: 5.34159, g_mul_loss: 0.00000, g_loss: 5.34159\n",
            "Epoch: [  76/ 500] d_bin_loss: 0.58514, d_mul_loss: 0.01034, d_loss: 0.59548, g_bin_loss: 2.86657, g_mul_loss: 0.00001, g_loss: 2.86657\n",
            "Epoch: [  77/ 500] d_bin_loss: 0.04881, d_mul_loss: 0.00123, d_loss: 0.05004, g_bin_loss: 5.21515, g_mul_loss: 0.00001, g_loss: 5.21516\n",
            "Epoch: [  78/ 500] d_bin_loss: 0.11563, d_mul_loss: 0.00160, d_loss: 0.11724, g_bin_loss: 4.94574, g_mul_loss: 0.00001, g_loss: 4.94574\n",
            "Epoch: [  79/ 500] d_bin_loss: 0.03591, d_mul_loss: 0.00212, d_loss: 0.03803, g_bin_loss: 5.99412, g_mul_loss: 0.00001, g_loss: 5.99413\n",
            "Epoch: [  80/ 500] d_bin_loss: 0.10647, d_mul_loss: 0.00012, d_loss: 0.10660, g_bin_loss: 5.19772, g_mul_loss: 0.00000, g_loss: 5.19772\n",
            "Epoch: [  81/ 500] d_bin_loss: 0.01904, d_mul_loss: 0.00011, d_loss: 0.01915, g_bin_loss: 6.22686, g_mul_loss: 0.00000, g_loss: 6.22686\n",
            "Epoch: [  82/ 500] d_bin_loss: 0.04823, d_mul_loss: 0.00300, d_loss: 0.05124, g_bin_loss: 6.27786, g_mul_loss: 0.00000, g_loss: 6.27787\n",
            "Epoch: [  83/ 500] d_bin_loss: 0.02759, d_mul_loss: 0.00302, d_loss: 0.03061, g_bin_loss: 6.01157, g_mul_loss: 0.00001, g_loss: 6.01158\n",
            "Epoch: [  84/ 500] d_bin_loss: 0.08335, d_mul_loss: 0.00087, d_loss: 0.08422, g_bin_loss: 5.27115, g_mul_loss: 0.00000, g_loss: 5.27115\n",
            "Epoch: [  85/ 500] d_bin_loss: 0.01911, d_mul_loss: 0.00032, d_loss: 0.01943, g_bin_loss: 6.16409, g_mul_loss: 0.00001, g_loss: 6.16409\n",
            "Epoch: [  86/ 500] d_bin_loss: 0.08564, d_mul_loss: 0.00091, d_loss: 0.08655, g_bin_loss: 4.79285, g_mul_loss: 0.00001, g_loss: 4.79286\n",
            "Epoch: [  87/ 500] d_bin_loss: 0.31377, d_mul_loss: 0.01195, d_loss: 0.32572, g_bin_loss: 5.05983, g_mul_loss: 0.00001, g_loss: 5.05984\n",
            "Epoch: [  88/ 500] d_bin_loss: 0.10740, d_mul_loss: 0.00112, d_loss: 0.10852, g_bin_loss: 6.24674, g_mul_loss: 0.00001, g_loss: 6.24674\n",
            "Epoch: [  89/ 500] d_bin_loss: 0.09959, d_mul_loss: 0.00049, d_loss: 0.10008, g_bin_loss: 5.25775, g_mul_loss: 0.00000, g_loss: 5.25775\n",
            "Epoch: [  90/ 500] d_bin_loss: 0.02788, d_mul_loss: 0.00015, d_loss: 0.02803, g_bin_loss: 5.83813, g_mul_loss: 0.00000, g_loss: 5.83813\n",
            "Epoch: [  91/ 500] d_bin_loss: 0.07576, d_mul_loss: 0.00228, d_loss: 0.07803, g_bin_loss: 5.37051, g_mul_loss: 0.00001, g_loss: 5.37053\n",
            "Epoch: [  92/ 500] d_bin_loss: 0.02694, d_mul_loss: 0.02490, d_loss: 0.05184, g_bin_loss: 5.56380, g_mul_loss: 0.00001, g_loss: 5.56381\n",
            "Epoch: [  93/ 500] d_bin_loss: 0.13487, d_mul_loss: 0.00524, d_loss: 0.14011, g_bin_loss: 4.02753, g_mul_loss: 0.00001, g_loss: 4.02754\n",
            "Epoch: [  94/ 500] d_bin_loss: 0.02499, d_mul_loss: 0.01754, d_loss: 0.04253, g_bin_loss: 6.74148, g_mul_loss: 0.00000, g_loss: 6.74148\n",
            "Epoch: [  95/ 500] d_bin_loss: 0.13683, d_mul_loss: 0.00048, d_loss: 0.13731, g_bin_loss: 6.48352, g_mul_loss: 0.00000, g_loss: 6.48353\n",
            "Epoch: [  96/ 500] d_bin_loss: 0.08054, d_mul_loss: 0.00069, d_loss: 0.08124, g_bin_loss: 5.28991, g_mul_loss: 0.00000, g_loss: 5.28991\n",
            "Epoch: [  97/ 500] d_bin_loss: 0.00148, d_mul_loss: 2.18097, d_loss: 2.18245, g_bin_loss: 9.50725, g_mul_loss: 0.02542, g_loss: 9.53267\n",
            "Epoch: [  98/ 500] d_bin_loss: 0.05132, d_mul_loss: 0.50851, d_loss: 0.55983, g_bin_loss: 7.60318, g_mul_loss: 0.00264, g_loss: 7.60582\n",
            "Epoch: [  99/ 500] d_bin_loss: 0.16906, d_mul_loss: 0.00207, d_loss: 0.17113, g_bin_loss: 5.27674, g_mul_loss: 0.00002, g_loss: 5.27676\n",
            "Epoch: [ 100/ 500] d_bin_loss: 0.18521, d_mul_loss: 0.00125, d_loss: 0.18646, g_bin_loss: 4.38342, g_mul_loss: 0.00000, g_loss: 4.38343\n",
            "Epoch: [ 101/ 500] d_bin_loss: 0.13179, d_mul_loss: 0.00014, d_loss: 0.13193, g_bin_loss: 6.79072, g_mul_loss: 0.00000, g_loss: 6.79072\n",
            "Epoch: [ 102/ 500] d_bin_loss: 0.05507, d_mul_loss: 0.00275, d_loss: 0.05782, g_bin_loss: 5.42347, g_mul_loss: 0.00001, g_loss: 5.42347\n",
            "Epoch: [ 103/ 500] d_bin_loss: 0.04454, d_mul_loss: 0.00668, d_loss: 0.05122, g_bin_loss: 5.13639, g_mul_loss: 0.00001, g_loss: 5.13639\n",
            "Epoch: [ 104/ 500] d_bin_loss: 0.08750, d_mul_loss: 0.02856, d_loss: 0.11607, g_bin_loss: 5.82079, g_mul_loss: 0.00002, g_loss: 5.82081\n",
            "Epoch: [ 105/ 500] d_bin_loss: 3.25673, d_mul_loss: 0.06102, d_loss: 3.31775, g_bin_loss: 7.33905, g_mul_loss: 0.00010, g_loss: 7.33916\n",
            "Epoch: [ 106/ 500] d_bin_loss: 0.04546, d_mul_loss: 0.00007, d_loss: 0.04553, g_bin_loss: 5.73930, g_mul_loss: 0.00000, g_loss: 5.73930\n",
            "Epoch: [ 107/ 500] d_bin_loss: 0.07480, d_mul_loss: 0.00638, d_loss: 0.08119, g_bin_loss: 6.40510, g_mul_loss: 0.00001, g_loss: 6.40511\n",
            "Epoch: [ 108/ 500] d_bin_loss: 0.10965, d_mul_loss: 0.02020, d_loss: 0.12985, g_bin_loss: 5.08133, g_mul_loss: 0.00002, g_loss: 5.08135\n",
            "Epoch: [ 109/ 500] d_bin_loss: 0.05969, d_mul_loss: 0.00037, d_loss: 0.06006, g_bin_loss: 5.50320, g_mul_loss: 0.00000, g_loss: 5.50320\n",
            "Epoch: [ 110/ 500] d_bin_loss: 0.05371, d_mul_loss: 0.00207, d_loss: 0.05578, g_bin_loss: 5.62791, g_mul_loss: 0.00001, g_loss: 5.62792\n",
            "Epoch: [ 111/ 500] d_bin_loss: 0.09479, d_mul_loss: 0.00445, d_loss: 0.09924, g_bin_loss: 4.93159, g_mul_loss: 0.00001, g_loss: 4.93160\n",
            "Epoch: [ 112/ 500] d_bin_loss: 0.11083, d_mul_loss: 0.01068, d_loss: 0.12151, g_bin_loss: 4.97475, g_mul_loss: 0.00001, g_loss: 4.97476\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}