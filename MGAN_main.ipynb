{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MGAN_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyPGs+isOBU89FY5jNcon2XT"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkwh_DjlmCyr",
        "outputId": "87dad894-a5bd-4967-e1ec-dcbe29c25cee"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import argparse\n",
        "import math\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/MyDrive/Colab_Notebooks')\n",
        "import models\n",
        "\n",
        "from argparse import ArgumentParser"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LONWnk7EkuE4"
      },
      "source": [
        "batch_norm = partial(tf.compat.v1.layers.batch_normalization,\n",
        "                     momentum=0.99,\n",
        "                     trainable=True,\n",
        "                     epsilon=1e-5,\n",
        "                     scale=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfCdRQlxlFQB"
      },
      "source": [
        "class Prior(object):\n",
        "    def __init__(self, type):\n",
        "        self.type = type\n",
        "\n",
        "    def sample(self, shape):\n",
        "        if self.type == \"uniform\":\n",
        "            return np.random.uniform(-1.0, 1.0, shape)\n",
        "        else:\n",
        "            return np.random.normal(0, 1, shape)\n",
        "\n",
        "def conv_out_size_same(size, stride):\n",
        "    return int(math.ceil(float(size) / float(stride)))\n",
        "\n",
        "def make_batches(size, batch_size):\n",
        "    '''Returns a list of batch indices (tuples of indices).\n",
        "    '''\n",
        "    return [(i, min(size, i + batch_size)) for i in range(0, size, batch_size)]\n",
        "\n",
        "def create_image_grid(x, img_size, tile_shape):\n",
        "    assert (x.shape[0] == tile_shape[0] * tile_shape[1])\n",
        "    assert (x[0].shape == img_size)\n",
        "\n",
        "    img = np.zeros((img_size[0] * tile_shape[0] + tile_shape[0] - 1,\n",
        "                    img_size[1] * tile_shape[1] + tile_shape[1] - 1,\n",
        "                    3))\n",
        "\n",
        "    for t in range(x.shape[0]):\n",
        "        i, j = t // tile_shape[1], t % tile_shape[1]\n",
        "        img[i * img_size[0] + i : (i + 1) * img_size[0] + i, j * img_size[1] + j : (j + 1) * img_size[1] + j] = x[t]\n",
        "\n",
        "    return img"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik-bA-NjlVWq"
      },
      "source": [
        "def linear(input, output_dim, scope='linear', stddev=0.01):\n",
        "    norm = tf.random_normal_initializer(stddev=stddev)\n",
        "    const = tf.constant_initializer(0.0)\n",
        "    with tf.variable_scope(scope):\n",
        "        w = tf.get_variable('weights', [input.get_shape()[1], output_dim], initializer=norm)\n",
        "        b = tf.get_variable('biases', [output_dim], initializer=const)\n",
        "        return tf.matmul(input, w) + b"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3CDU8DR6a1z"
      },
      "source": [
        "def conv2d(input_, output_dim,\n",
        "           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
        "           name=\"conv2d\"):\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable('weights', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
        "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "        # conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
        "\n",
        "        return tf.nn.bias_add(conv, biases)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeoQ1YNm5Ys5"
      },
      "source": [
        "def deconv2d(input_, output_shape,\n",
        "             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
        "             name=\"deconv2d\", with_w=False):\n",
        "    with tf.variable_scope(name):\n",
        "        # filter : [height, width, output_channels, in_channels]\n",
        "        w = tf.get_variable('weights', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
        "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
        "\n",
        "        try:\n",
        "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n",
        "                                            strides=[1, d_h, d_w, 1])\n",
        "\n",
        "        # Support for versions of TensorFlow before 0.7.0\n",
        "        except AttributeError:\n",
        "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n",
        "                                    strides=[1, d_h, d_w, 1])\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_shape[-1]],\n",
        "                                 initializer=tf.constant_initializer(0.0))\n",
        "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
        "\n",
        "        if with_w:\n",
        "            return deconv, w, biases\n",
        "        else:\n",
        "            return deconv"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5OkeAuY6QPr"
      },
      "source": [
        "def lrelu(x, alpha=0.2):\n",
        "    return tf.maximum(x, alpha * x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "flygcpRlv7QU",
        "outputId": "519d1f15-908d-453f-a042-aef50fb994af"
      },
      "source": [
        "!pip install Pillow==5.0.0\n",
        "!pip install scipy==1.0.1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==5.0.0\n",
            "  Downloading Pillow-5.0.0.tar.gz (14.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2 MB 162 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow\n",
            "  Building wheel for Pillow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow: filename=Pillow-5.0.0-cp37-cp37m-linux_x86_64.whl size=1066854 sha256=d21d2f961f11847826bf424016f3a95f81cd8ab2d1e5e5a2f6fb56bf2b07e581\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/e9/3f/e1cf5b199032b95130c1e68222992ce83a78a51bdba0577a3b\n",
            "Successfully built Pillow\n",
            "Installing collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires pillow>=5.3.0, but you have pillow 5.0.0 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 5.0.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-5.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.0.1\n",
            "  Downloading scipy-1.0.1.tar.gz (15.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.5 MB 161 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: scipy\n",
            "  Building wheel for scipy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scipy: filename=scipy-1.0.1-cp37-cp37m-linux_x86_64.whl size=39978473 sha256=54d1821cdfff0734578e3625e684ac5638d1e719cd1e95fa35c4faeec97377eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/78/6c/81857cb7b297db9725704452fd48e7ef25e0fc395642aee63c\n",
            "Successfully built scipy\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc3 3.11.2 requires scipy>=1.2.0, but you have scipy 1.0.1 which is incompatible.\n",
            "plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.0.1 which is incompatible.\n",
            "cvxpy 1.0.31 requires scipy>=1.1.0, but you have scipy 1.0.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt3TDrww10xm",
        "outputId": "b01ff5ee-cd05-4758-8c8b-2ffec77083a2"
      },
      "source": [
        "#from models import MGAN\n",
        "\n",
        "\n",
        "class MGAN(object):\n",
        "    \"\"\"Mixture Generative Adversarial Nets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_name='MGAN',\n",
        "                 beta=1.0,\n",
        "                 num_z=128,\n",
        "                 num_gens=4,\n",
        "                 d_batch_size=64,\n",
        "                 g_batch_size=32,\n",
        "                 z_prior=\"uniform\",\n",
        "                 same_input=True,\n",
        "                 learning_rate=0.0002,\n",
        "                 img_size=(32, 32, 3),  # (height, width, channels)\n",
        "                 num_conv_layers=3,\n",
        "                 num_gen_feature_maps=128,  # number of feature maps of generator\n",
        "                 num_dis_feature_maps=128,  # number of feature maps of discriminator\n",
        "                 sample_fp=None,\n",
        "                 sample_by_gen_fp=None,\n",
        "                 num_epochs=5,\n",
        "                 random_seed=6789):\n",
        "        self.beta = beta\n",
        "        self.num_z = num_z\n",
        "        self.num_gens = num_gens\n",
        "        self.d_batch_size = d_batch_size\n",
        "        self.g_batch_size = g_batch_size\n",
        "        self.z_prior = Prior(z_prior)\n",
        "        self.same_input = same_input\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.img_size = img_size\n",
        "        self.num_conv_layers = num_conv_layers\n",
        "        self.num_gen_feature_maps = num_gen_feature_maps\n",
        "        self.num_dis_feature_maps = num_dis_feature_maps\n",
        "        self.sample_fp = sample_fp\n",
        "        self.sample_by_gen_fp = sample_by_gen_fp\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "    def _init(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "        # TensorFlow's initialization\n",
        "        self.tf_graph = tf.Graph()\n",
        "        self.tf_config = tf.ConfigProto()\n",
        "        self.tf_config.gpu_options.allow_growth = True\n",
        "        self.tf_config.log_device_placement = False\n",
        "        self.tf_config.allow_soft_placement = True\n",
        "        self.tf_session = tf.Session(config=self.tf_config, graph=self.tf_graph)\n",
        "\n",
        "        np.random.seed(self.random_seed)\n",
        "        with self.tf_graph.as_default():\n",
        "            tf.set_random_seed(self.random_seed)\n",
        "\n",
        "    def _build_model(self):\n",
        "        arr = np.array([i // self.g_batch_size for i in range(self.g_batch_size * self.num_gens)])\n",
        "        d_mul_labels = tf.constant(arr, dtype=tf.int32)\n",
        "\n",
        "        self.x = tf.placeholder(tf.float32, [None,\n",
        "                                             self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                                name=\"real_data\")\n",
        "        self.z = tf.placeholder(tf.float32, [self.g_batch_size * self.num_gens, self.num_z], name='noise')\n",
        "\n",
        "        # create generator G\n",
        "        self.g = self._create_generator(self.z)\n",
        "\n",
        "        # create sampler to generate samples\n",
        "        self.sampler = self._create_generator(self.z, train=False, reuse=True)\n",
        "\n",
        "        # create discriminator D\n",
        "        d_bin_x_logits, d_mul_x_logits = self._create_discriminator(self.x)\n",
        "        d_bin_g_logits, d_mul_g_logits = self._create_discriminator(self.g, reuse=True)\n",
        "\n",
        "        # define loss functions\n",
        "        self.d_bin_x_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=d_bin_x_logits, labels=tf.ones_like(d_bin_x_logits)),\n",
        "            name='d_bin_x_loss')\n",
        "        self.d_bin_g_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=d_bin_g_logits, labels=tf.zeros_like(d_bin_g_logits)),\n",
        "            name='d_bin_g_loss')\n",
        "        self.d_bin_loss = tf.add(self.d_bin_x_loss, self.d_bin_g_loss, name='d_bin_loss')\n",
        "        self.d_mul_loss = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                logits=d_mul_g_logits, labels=d_mul_labels),\n",
        "            name=\"d_mul_loss\")\n",
        "        self.d_loss = tf.add(self.d_bin_loss, self.d_mul_loss, name=\"d_loss\")\n",
        "\n",
        "        self.g_bin_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=d_bin_g_logits, labels=tf.ones_like(d_bin_g_logits)),\n",
        "            name=\"g_bin_loss\")\n",
        "        self.g_mul_loss = tf.multiply(self.beta, self.d_mul_loss, name='g_mul_loss')\n",
        "        self.g_loss = tf.add(self.g_bin_loss, self.g_mul_loss, name=\"g_loss\")\n",
        "\n",
        "        # create optimizers\n",
        "        self.d_opt = self._create_optimizer(self.d_loss, scope='discriminator',\n",
        "                                            lr=self.learning_rate)\n",
        "        self.g_opt = self._create_optimizer(self.g_loss, scope='generator',\n",
        "                                            lr=self.learning_rate)\n",
        "\n",
        "    def _create_generator(self, z, train=True, reuse=False, name=\"generator\"):\n",
        "        out_size = [(conv_out_size_same(self.img_size[0], 2),\n",
        "                     conv_out_size_same(self.img_size[1], 2),\n",
        "                     self.num_gen_feature_maps)]\n",
        "        for i in range(self.num_conv_layers - 1):\n",
        "            out_size = [(conv_out_size_same(out_size[0][0], 2),\n",
        "                         conv_out_size_same(out_size[0][1], 2),\n",
        "                         out_size[0][2] * 2)] + out_size\n",
        "\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            if reuse:\n",
        "                scope.reuse_variables()\n",
        "\n",
        "            z_split = tf.split(z, self.num_gens, axis=0)\n",
        "            h0 = []\n",
        "            for i, var in enumerate(z_split):\n",
        "                h0.append(tf.nn.relu(batch_norm(linear(var, out_size[0][0] * out_size[0][1] * out_size[0][2],\n",
        "                                                       scope='g_h0_linear{}'.format(i), stddev=0.02),\n",
        "                                                training=train #,scope=\"g_h0_bn{}\".format(i)\n",
        "                                                ),\n",
        "                                     name=\"g_h0_relu{}\".format(i)))\n",
        "\n",
        "            h = []\n",
        "            for var in h0:\n",
        "                h.append(tf.reshape(var, [self.g_batch_size, out_size[0][0], out_size[0][1], out_size[0][2]]))\n",
        "            h = tf.concat(h, axis=0, name=\"g_h0_relu\")\n",
        "\n",
        "            for i in range(1, self.num_conv_layers):\n",
        "                h = tf.nn.relu(\n",
        "                    batch_norm(\n",
        "                        deconv2d(h,\n",
        "                                 [self.g_batch_size  * self.num_gens, out_size[i][0], out_size[i][1], out_size[i][2]],\n",
        "                                 stddev=0.02, name=\"g_h{}_deconv\".format(i)),\n",
        "                        training=train,\n",
        "                        center=False\n",
        "                        #,scope=\"g_h{}_bn\".format(i)\n",
        "                        ),\n",
        "                    name=\"g_h{}_relu\".format(i))\n",
        "\n",
        "            g_out = tf.nn.tanh(\n",
        "                deconv2d(h,\n",
        "                         [self.g_batch_size * self.num_gens, self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                         stddev=0.02, name=\"g_out_deconv\"),\n",
        "                name=\"g_out_tanh\")\n",
        "            return g_out\n",
        "\n",
        "    def _create_discriminator(self, x, train=True, reuse=False, name=\"discriminator\"):\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            if reuse:\n",
        "                scope.reuse_variables()\n",
        "\n",
        "            h = x\n",
        "            for i in range(self.num_conv_layers):\n",
        "                h = lrelu(batch_norm(conv2d(h, self.num_dis_feature_maps * (2 ** i),\n",
        "                                            stddev=0.02, name=\"d_h{}_conv\".format(i)),\n",
        "                                     training=train\n",
        "                                     #,\n",
        "                                     #scope=\"d_bn{}\".format(i)\n",
        "                                     ))\n",
        "\n",
        "            dim = h.get_shape()[1:].num_elements()\n",
        "            h = tf.reshape(h, [-1, dim])\n",
        "            d_bin_logits = linear(h, 1, scope='d_bin_logits')\n",
        "            d_mul_logits = linear(h, self.num_gens, scope='d_mul_logits')\n",
        "        return d_bin_logits, d_mul_logits\n",
        "\n",
        "    def _create_optimizer(self, loss, scope, lr):\n",
        "        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
        "        opt = tf.train.AdamOptimizer(lr, beta1=0.5)\n",
        "        grads = opt.compute_gradients(loss, var_list=params)\n",
        "        train_op = opt.apply_gradients(grads)\n",
        "        return train_op\n",
        "\n",
        "    def fit(self, x):\n",
        "        if (not hasattr(self, 'epoch')) or self.epoch == 0:\n",
        "            self._init()\n",
        "            with self.tf_graph.as_default():\n",
        "                self._build_model()\n",
        "                self.tf_session.run(tf.global_variables_initializer())\n",
        "\n",
        "        num_data = x.shape[0] - x.shape[0] % self.d_batch_size\n",
        "        batches = make_batches(num_data, self.d_batch_size)\n",
        "        best_is = 0.0\n",
        "        while (self.epoch < self.num_epochs):\n",
        "            for batch_idx, (batch_start, batch_end) in enumerate(batches):\n",
        "                batch_size = batch_end - batch_start\n",
        "\n",
        "                x_batch = x[batch_start:batch_end]\n",
        "                if self.same_input:\n",
        "                    z_batch = self.z_prior.sample([self.g_batch_size, self.num_z]).astype(np.float32)\n",
        "                    z_batch = np.vstack([z_batch] * self.num_gens)\n",
        "                else:\n",
        "                    z_batch = self.z_prior.sample([self.g_batch_size * self.num_gens, self.num_z]).astype(np.float32)\n",
        "\n",
        "                # update discriminator D\n",
        "                d_bin_loss, d_mul_loss, d_loss, _ = self.tf_session.run(\n",
        "                    [self.d_bin_loss, self.d_mul_loss, self.d_loss, self.d_opt],\n",
        "                    feed_dict={self.x: x_batch, self.z: z_batch})\n",
        "\n",
        "                # update generator G\n",
        "                g_bin_loss, g_mul_loss, g_loss, _ = self.tf_session.run(\n",
        "                    [self.g_bin_loss, self.g_mul_loss, self.g_loss, self.g_opt],\n",
        "                    feed_dict={self.z: z_batch})\n",
        "\n",
        "            self.epoch += 1\n",
        "            print(\"Epoch: [%4d/%4d] d_bin_loss: %.5f, d_mul_loss: %.5f, d_loss: %.5f,\"\n",
        "                  \" g_bin_loss: %.5f, g_mul_loss: %.5f, g_loss: %.5f\" % (self.epoch, self.num_epochs,\n",
        "                                d_bin_loss, d_mul_loss, d_loss, g_bin_loss, g_mul_loss, g_loss))\n",
        "            self._samples(self.sample_fp.format(epoch=self.epoch+1))\n",
        "            self._samples_by_gen(self.sample_by_gen_fp.format(epoch=self.epoch+1))\n",
        "\n",
        "    def _generate(self, num_samples=100):\n",
        "        sess = self.tf_session\n",
        "        batch_size = self.g_batch_size * self.num_gens\n",
        "        num = ((num_samples - 1) // batch_size + 1) * batch_size\n",
        "        z = self.z_prior.sample([num, self.num_z]).astype(np.float32)\n",
        "        x = np.zeros([num, self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                     dtype=np.float32)\n",
        "        batches = make_batches(num, batch_size)\n",
        "        for batch_idx, (batch_start, batch_end) in enumerate(batches):\n",
        "            z_batch = z[batch_start:batch_end]\n",
        "            x[batch_start:batch_end] = sess.run(self.sampler,\n",
        "                                                feed_dict={self.z: z_batch})\n",
        "        idx = np.random.permutation(num)[:num_samples]\n",
        "        x = (x[idx] + 1.0) / 2.0\n",
        "        return x\n",
        "\n",
        "    def _samples(self, filepath, tile_shape=(10, 10)):\n",
        "        if not os.path.exists(os.path.dirname(filepath)):\n",
        "            os.makedirs(os.path.dirname(filepath))\n",
        "\n",
        "        num_samples = tile_shape[0] * tile_shape[1]\n",
        "        x = self._generate(num_samples)\n",
        "        imgs = create_image_grid(x, img_size=self.img_size, tile_shape=tile_shape)\n",
        "        import scipy.misc\n",
        "        scipy.misc.imsave(filepath, imgs)\n",
        "\n",
        "    def _samples_by_gen(self, filepath):\n",
        "        if not os.path.exists(os.path.dirname(filepath)):\n",
        "            os.makedirs(os.path.dirname(filepath))\n",
        "\n",
        "        num_samples = self.num_gens * 10\n",
        "        tile_shape = (self.num_gens, 10)\n",
        "\n",
        "        sess = self.tf_session\n",
        "        img_per_gen = num_samples // self.num_gens\n",
        "        x = np.zeros([num_samples, self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                     dtype=np.float32)\n",
        "        for i in range(0, img_per_gen, self.g_batch_size):\n",
        "            z_batch = self.z_prior.sample([self.g_batch_size * self.num_gens, self.num_z]).astype(np.float32)\n",
        "            samples = sess.run(self.sampler, feed_dict={self.z: z_batch})\n",
        "\n",
        "            for gen in range(self.num_gens):\n",
        "                x[gen * img_per_gen + i:gen * img_per_gen + min(i + self.g_batch_size, img_per_gen)] = \\\n",
        "                    samples[\n",
        "                    gen * self.g_batch_size:gen * self.g_batch_size + min(self.g_batch_size, img_per_gen)]\n",
        "\n",
        "        x = (x + 1.0) / 2.0\n",
        "        imgs = create_image_grid(x, img_size=self.img_size, tile_shape=tile_shape)\n",
        "        import scipy.misc\n",
        "        scipy.misc.imsave(filepath, imgs)\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    tmp = pickle.load(open(\"/content/drive/MyDrive/Colab_Notebooks/cifar10_train.pkl\", \"rb\"))\n",
        "    x_train = tmp['data'].astype(np.float32).reshape([-1, 32, 32, 3]) / 127.5 - 1.\n",
        "    model = MGAN(\n",
        "        num_z=FLAGS.num_z,\n",
        "        beta=FLAGS.beta,\n",
        "        num_gens=FLAGS.num_gens,\n",
        "        d_batch_size=FLAGS.d_batch_size,\n",
        "        g_batch_size=FLAGS.g_batch_size,\n",
        "        z_prior=FLAGS.z_prior,\n",
        "        learning_rate=FLAGS.learning_rate,\n",
        "        img_size=(32, 32, 3),\n",
        "        num_conv_layers=FLAGS.num_conv_layers,\n",
        "        num_gen_feature_maps=FLAGS.num_gen_feature_maps,\n",
        "        num_dis_feature_maps=FLAGS.num_dis_feature_maps,\n",
        "        num_epochs=FLAGS.num_epochs,\n",
        "        sample_fp=\"samples/samples_{epoch:04d}.png\",\n",
        "        sample_by_gen_fp=\"samples_by_gen/samples_{epoch:04d}.png\",\n",
        "        random_seed=6789)\n",
        "    model.fit(x_train)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--num_z', type=int, default=100,\n",
        "                        help='Number of latent units.')\n",
        "    parser.add_argument('--beta', type=float, default=0.01,\n",
        "                        help='Diversity parameter beta.')\n",
        "    parser.add_argument('--num_gens', type=int, default=10,\n",
        "                        help='Number of generators.')\n",
        "    parser.add_argument('--d_batch_size', type=int, default=64,\n",
        "                        help='Minibatch size for the discriminator.')\n",
        "    parser.add_argument('--g_batch_size', type=int, default=12,\n",
        "                        help='Minibatch size for the generators.')\n",
        "    parser.add_argument('--z_prior', type=str, default=\"uniform\",\n",
        "                        help='Prior distribution of the noise (uniform/gaussian).')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.0002,\n",
        "                        help='Learning rate.')\n",
        "    parser.add_argument('--num_conv_layers', type=int, default=3,\n",
        "                        help='Number of convolutional layers.')\n",
        "    parser.add_argument('--num_gen_feature_maps', type=int, default=128,\n",
        "                        help='Number of feature maps of Generator.')\n",
        "    parser.add_argument('--num_dis_feature_maps', type=int, default=128,\n",
        "                        help='Number of feature maps of Discriminator.')\n",
        "    parser.add_argument('--num_epochs', type=int, default=500,\n",
        "                        help='Number of epochs.')\n",
        "    FLAGS, unparsed = parser.parse_known_args()\n",
        "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [  17/ 500] d_bin_loss: 0.34571, d_mul_loss: 0.02713, d_loss: 0.37285, g_bin_loss: 1.99624, g_mul_loss: 0.00004, g_loss: 1.99628\n",
            "Epoch: [  18/ 500] d_bin_loss: 0.39758, d_mul_loss: 0.05050, d_loss: 0.44808, g_bin_loss: 2.12974, g_mul_loss: 0.00006, g_loss: 2.12981\n",
            "Epoch: [  19/ 500] d_bin_loss: 0.38838, d_mul_loss: 0.00748, d_loss: 0.39586, g_bin_loss: 2.56967, g_mul_loss: 0.00006, g_loss: 2.56973\n",
            "Epoch: [  20/ 500] d_bin_loss: 0.55270, d_mul_loss: 0.01155, d_loss: 0.56426, g_bin_loss: 2.01715, g_mul_loss: 0.00006, g_loss: 2.01721\n",
            "Epoch: [  21/ 500] d_bin_loss: 0.95568, d_mul_loss: 0.05494, d_loss: 1.01062, g_bin_loss: 3.55065, g_mul_loss: 0.00003, g_loss: 3.55069\n",
            "Epoch: [  22/ 500] d_bin_loss: 0.36657, d_mul_loss: 0.01735, d_loss: 0.38392, g_bin_loss: 3.23083, g_mul_loss: 0.00005, g_loss: 3.23088\n",
            "Epoch: [  23/ 500] d_bin_loss: 1.37745, d_mul_loss: 0.03657, d_loss: 1.41402, g_bin_loss: 1.45535, g_mul_loss: 0.00003, g_loss: 1.45538\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}